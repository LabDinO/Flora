{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HERE WE WILL BE DEFINING THE BEST PARAMETERS \n",
    "# BRIGTHNESS\n",
    "# Pixels quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import wave\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pydub import AudioSegment\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Spectrogram Function ONLY IMAGE NO WHITE LAYOUT\n",
    "def plot_spectrogram(Y, sr, hop_length, y_axis=\"linear\", title=\"Spectrogram\"):\n",
    "    #(400/80, 320/80), dpi=80  TESTE COM DPI PARA DELIMITAÇÃO DE PIXELS\n",
    "    dpi = 100  # You can adjust this value based on your needs\n",
    "    width, height = 224 / dpi, 224 / dpi\n",
    "    plt.figure(figsize=(width, height), dpi=dpi)\n",
    "    plt.axis('off')  # Desative os eixos\n",
    "    plt.margins(0, 0) #define margens pra zero\n",
    "    plt.ylim(20000, 48000) #Limiiting the low frequency\n",
    "\n",
    "    # Set the desired frequency range (Deep Convolutional Neural Networks for Detecting Dolphin Echolocation Clicks)\n",
    "    fmin = 3000  # 3kHz\n",
    "    fmax = 144000  # 144kH\n",
    "    librosa.display.specshow(Y,\n",
    "                             sr=sr,\n",
    "                             hop_length=hop_length,\n",
    "                             x_axis=\"time\",\n",
    "                             y_axis=y_axis,\n",
    "                             cmap='gray',\n",
    "                             fmin=fmin,\n",
    "                             fmax=fmax)\n",
    "\n",
    "    plt.clim(-60, 10) #Definido por bibliografia (Deep Convolutional Neural Networks for Detecting Dolphin Echolocation Clicks)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "#Audio parameters\n",
    "\n",
    "FRAME_SIZE = 1024 #samples\n",
    "HOP_SIZE = 512 #samples\n",
    "sr=96000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def click_images(file,file_raw,audio_data,sample_rate):\n",
    "     \n",
    "    for x in range (4): #Loop que vai passar por cada um dos três canais\n",
    "        spectrogram = audio_data[x] # loc no canal do audio file\n",
    "        ch = file.loc[(file['label'] == 1)] #pegando apenas os que tem cliques\n",
    "        df = ch.loc[(ch['channel'] == x +1)]  #Vendo o canal correspondentes no csv \n",
    "        for i in  range (len(df)):  #LOOP PASSA LINHA A LINHA DE CADA CANAL\n",
    "                t_start = df['begin_time'].iloc[i] # begin time i\n",
    "                t_stop = df['end_time'].iloc[i] # end time i\n",
    "            \n",
    "                #Agora temos que ver se o tempo entre t_start e t_stop é par\n",
    "                t = t_stop - t_start\n",
    "                n = int(t // 2)  #n é o número de espectrogramas\n",
    "\n",
    "                if n == 0: #Para ajeitar calls menores que 2 segundos\n",
    "                    n = 1\n",
    "\n",
    "                for j in range (n):\n",
    "                    t_start = math.floor(t_start) #arredonda t_start pra baixo\n",
    "                    t_stop = t_start + 2  #atualiza t_stop pra pegar o espectrograma de tamanho 2 segundos\n",
    "                    audio_clip = audio_data[x][int(t_start * sr):int(t_stop * sr)]\n",
    "                    #Agora a gente pega e recorta os espectrogramas \n",
    "                    S_ch_raw1 = librosa.stft(audio_clip, n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
    "                    Y_scale = np.abs(S_ch_raw1) ** 2\n",
    "                    Y_log_scale1 = librosa.power_to_db(Y_scale)\n",
    "                    plot_spectrogram(Y_log_scale1 , sr, HOP_SIZE,y_axis=\"linear\", title=\"Yes Dolphins\")\n",
    "                    \n",
    "                    t_start = t_start + 2  #vai passando pro proximo espectrograma até dar n, porque n é a quantidade de espectrogramas que tem no intervalo anotado no csv\n",
    "\n",
    "                    output_folder = 'D:/IMAGES_CNN/testing_parameters/positive'\n",
    "                    # Salve a figura no formato desejado (por exemplo, PNG)\n",
    "                    output_path = os.path.join(output_folder, f'chan_{x+1}_line_{i+1}_spec_{j+1}_{ch[\"filename\"].iloc[i]}_{df[\"begin_time\"].iloc[i]}_{df[\"end_time\"].iloc[i]}.png')\n",
    "                    plt.savefig(output_path,bbox_inches='tight', pad_inches=0)\n",
    "                    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "###ANOTTATION FILE###\n",
    "#####################\n",
    "test_file =  pd.read_csv('C:/Users/flora/OneDrive/Documentos/MESTRADO_UFSC/rotinas/python/Flora/espectrogramas/labels_DeepVoice/annotation_train_clicks.csv')\n",
    "\n",
    "#Lists all the audio files names with no repeat\n",
    "audio_names = test_file['filename'].unique()\n",
    "\n",
    "\n",
    "\n",
    "#CRIAR UM LOOP QUE VÁ NA PASTA DOS AUDIOS E LEIA AUDIO POR AUDIO E FIQUE CRIANDO IMAGENS \n",
    "\n",
    "#Testando 1 file\n",
    "\n",
    "\n",
    "k = 0\n",
    "#the audio file\n",
    "file = test_file.loc[(test_file['filename'] == audio_names[k])]\n",
    "file_raw = f'D:/AUDIOS/train/{audio_names[k]}.wav'\n",
    "audio_data, sample_rate = librosa.load(file_raw, sr=None, mono=False)\n",
    "#negative_images(file, file_raw, audio_data, sample_rate)\n",
    "click_images(file,file_raw,audio_data,sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As imagens são do audio LPS1202017_MF_20171113_030719_394\n"
     ]
    }
   ],
   "source": [
    "print(f\"As imagens são do audio {audio_names[k]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
